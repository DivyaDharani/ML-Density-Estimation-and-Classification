{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('fashion_mnist.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file Platform: nt, Created on: Tue Feb  2 11:20:21 2021',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'trX': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'trY': array([[0., 0., 0., ..., 1., 1., 1.]]),\n",
       " 'tsX': array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.00392157, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]]),\n",
       " 'tsY': array([[0., 1., 1., ..., 1., 0., 1.]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trX = data['trX']\n",
    "trY = data['trY']\n",
    "tsX = data['tsX']\n",
    "tsY = data['tsY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction\n",
    "X_train = np.array([[sample.mean(), sample.std()] for sample in trX])\n",
    "X_test = np.array([[sample.mean(), sample.std()] for sample in tsX])\n",
    "y_train = np.reshape(trY, -1)\n",
    "y_test = np.reshape(tsY, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateByClass(X_train, y_train):\n",
    "    \"\"\"\n",
    "    This method separates the data according to the distinct class values\n",
    "    INPUT:\n",
    "        X_train: training data containing all the features\n",
    "        y_train: array of class labels corresponding to the training data\n",
    "    OUTPUT: \n",
    "        returns class-wise data (dictionary)\n",
    "    Note: The lengths of X_train and y_train should match\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train_by_class = {} #a dictionary\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        row = X_train[i]\n",
    "        classValue = y_train[i]\n",
    "        if(classValue not in X_train_by_class):\n",
    "            X_train_by_class[classValue] = []\n",
    "        X_train_by_class[classValue].append(row)\n",
    "    \n",
    "    return X_train_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMeanVarianceParams(dataset):\n",
    "    \"\"\"\n",
    "    This method calculates mean and variance for each feature column in the given dataset\n",
    "    INPUT: \n",
    "        dataset: dataset containing the features\n",
    "    OUTPUT: \n",
    "        returns list of mean and variance for each feature column\n",
    "    \"\"\"\n",
    "    meanVarianceParams = [[np.mean(feature), np.var(feature)] for feature in zip(*dataset)]\n",
    "    return meanVarianceParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMeanAndVarByClass(X_train_by_class, y_train):\n",
    "    \"\"\"\n",
    "    This method calculates mean and variance of the feature columns of the given dataset for each class separately\n",
    "    INPUT:\n",
    "        X_train_by_class: class-wise training data containing the features\n",
    "        y_train: array of class labels corresponding to the training data\n",
    "    OUTPUT:\n",
    "        returns list of mean and variance for each feature column for each class\n",
    "    \"\"\"\n",
    "    #model parameters - mean and variance\n",
    "    meanAndVarianceByClass = {}\n",
    "    for classValue in X_train_by_class.keys():\n",
    "        meanAndVarianceByClass[classValue] = calculateMeanVarianceParams(X_train_by_class[classValue])\n",
    "    return meanAndVarianceByClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(X_train_by_class, y_train):\n",
    "    \"\"\"\n",
    "    This method trains the Naive Bayes model by estimating the model parameters - mean and variance\n",
    "    INPUT:\n",
    "        X_train_by_class: class-wise training data containing the features\n",
    "        y_train: array of class labels corresponding to the training data\n",
    "    OUTPUT:\n",
    "        returns list of estimated model parameters by class and feature\n",
    "    \"\"\"\n",
    "    modelParams = calculateMeanAndVarByClass(X_train_by_class, y_train)\n",
    "    return modelParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Calculations and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateGaussianProbabilityDensity(x, mean, variance):\n",
    "    \"\"\"\n",
    "    This method calculates Gaussian Probability Density \n",
    "    INPUT:\n",
    "        x: data sample for which the probability density has to be calculated\n",
    "        mean: mean of the distribution belonging to x\n",
    "        variance: variance of the distribution belonging to x\n",
    "    OUTPUT:\n",
    "        returns the Gaussian probability density of the given data\n",
    "    \"\"\"\n",
    "    #here the values may exceed 1 as the output is the probability density, not probability\n",
    "    return (1 / math.sqrt(2*math.pi*variance)) * math.exp(-(math.pow(x-mean, 2) / (2 * variance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePriorProbabilities(X_train_by_class):\n",
    "    \"\"\"\n",
    "    This method calculates the prior probabilities P(y) of all the class labels in the given class-wise training data\n",
    "    INPUT: \n",
    "        X_train_by_class: class-wise training data containing the features\n",
    "    OUTPUT:\n",
    "        returns a list of prior probabilities for all the class labels\n",
    "    \"\"\"\n",
    "    prior = {}\n",
    "    totalLen = 0\n",
    "    for classValue, data in X_train_by_class.items():\n",
    "        totalLen += len(data)\n",
    "    for classValue, data in X_train_by_class.items():\n",
    "        prior[classValue] = len(data)/totalLen\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateClassProbabilities(modelParams, prior, testInput):\n",
    "    \n",
    "    \"\"\"\n",
    "    This method calculates conditional probabilities P(X,y) = P(y)*p(X/y) for the given data sample\n",
    "    P(y)*P(X|y) => P(y1)*P(X1|y1)*P(X2|y1) and P(y2)*P(X1|y2)*P(X2|y2) will be calculated\n",
    "    INPUT:\n",
    "        modelParams: list of estimated model paramters - mean and variance - for each class and feature\n",
    "        prior: list of prior probabilities of all the class labels\n",
    "        testInput: data sample for which the conditional probabilities have to be calculated\n",
    "    OUTPUT:\n",
    "        conditional probabilities for each class (dictionary)\n",
    "    \"\"\"\n",
    "    \n",
    "    classProbabilities = {}\n",
    "    \n",
    "    for classValue, params in modelParams.items():\n",
    "        classProbabilities[classValue] = prior[classValue]\n",
    "        for i in range(len(params)): #iterating through model parameters of each feature of this class\n",
    "            mean, variance = params[i]\n",
    "            x = testInput[i] #ith feature's value of the given test data\n",
    "            classProbabilities[classValue] *= calculateGaussianProbabilityDensity(x, mean, variance)\n",
    "    \n",
    "    return classProbabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(modelParams, prior, testInput):\n",
    "    \"\"\"\n",
    "    This method predicts the best class label for the given test data\n",
    "    INPUT:\n",
    "        modelParams: list of estimated model paramters - mean and variance - for each class and feature\n",
    "        prior: list of prior probabilities of all the class labels\n",
    "        testInput: data sample for which the class label prediction has to be made\n",
    "    OUTPUT:\n",
    "        returns the best fitting class label for the given data sample\n",
    "    \"\"\"\n",
    "    \n",
    "    classProbabilities = calculateClassProbabilities(modelParams, prior, testInput)\n",
    "    maxProb = -1\n",
    "    bestClass = None\n",
    "    for classValue, probability in classProbabilities.items():\n",
    "        if(probability > maxProb):\n",
    "            maxProb = probability\n",
    "            bestClass = classValue\n",
    "    return bestClass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(modelParams, X_train_by_class, X_test):\n",
    "    \"\"\"\n",
    "    This method performs the testing part of the model; returns predictions of the class labels of the given test data\n",
    "    INPUT:\n",
    "        modelParams: list of estimated model paramters - mean and variance - for each class and feature\n",
    "        X_train_by_class: class-wise training data containing the features\n",
    "        X_test:\n",
    "    OUTPUT:\n",
    "        returns a list containing the predictions of all the data samples in the testing set X_test\n",
    "    \"\"\"\n",
    "    prior = calculatePriorProbabilities(X_train_by_class)\n",
    "    predictions = []\n",
    "    for testInput in X_test:\n",
    "        predictions.append(predict(modelParams, prior, testInput))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.metrics import confusion_matrix, classification_report #for reporting the model performance\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(y_test, predictions):\n",
    "    \"\"\"\n",
    "    This model calculates the accuracy of a model\n",
    "    INPUT: \n",
    "        y_test: array of class labels of the testing data\n",
    "        predictions: array of the class labels predicted by the trained model\n",
    "    OUTPUT:\n",
    "        returns the 'accuracy' in terms of percentage\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if(predictions[i] == y_test[i]):\n",
    "            correct += 1\n",
    "    accuracy = (correct / len(y_test)) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printModelPerformance(y_test, predictions):\n",
    "    \"\"\"\n",
    "    This method prints the model performance - confusion matrix and classification report \n",
    "    INPUT: \n",
    "        y_test: array of class labels of the testing data\n",
    "        predictions: array of the class labels predicted by the trained model\n",
    "    OUTPUT:\n",
    "        prints the confusion matrix and the classification report\n",
    "    \"\"\"\n",
    "    try:\n",
    "        accuracy = getAccuracy(y_test, predictions)\n",
    "        print('\\nThe overall accuracy of the trained model is ' + str(accuracy) + '%')\n",
    "\n",
    "        print('\\nConfusion Matrix:')\n",
    "        print(confusion_matrix(y_test, predictions))\n",
    "        print(classification_report(y_test, predictions))\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performNaiveBayes(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This method performs all the steps required for Naive Bayes Classifier\n",
    "    INPUT:\n",
    "        X_train: training data with features \n",
    "        y_train: class labels of training data\n",
    "        X_test: testing data with features\n",
    "        y_test: class labels of testing data\n",
    "    OUTPUT:\n",
    "        prints the model performance after performing training and testing\n",
    "    \"\"\"\n",
    "    \n",
    "    print('\\n--- NAIVE BAYES CLASSIFIER ---')\n",
    "    \n",
    "    #Separating the training data by class\n",
    "    X_train_by_class = separateByClass(X_train, y_train)\n",
    "    \n",
    "    #Training the model by estimating the parameters\n",
    "    modelParams = trainModel(X_train_by_class, y_train)\n",
    "    \n",
    "    #Testing the model using the learned parameters\n",
    "    predictions = testModel(modelParams, X_train_by_class, X_test)\n",
    "    \n",
    "    \n",
    "    #Getting accuracy as a measure of model performance\n",
    "    printModelPerformance(y_test, predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NAIVE BAYES CLASSIFIER ---\n",
      "\n",
      "The overall accuracy of the trained model is 83.15%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[784 216]\n",
      " [121 879]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.78      0.82      1000\n",
      "         1.0       0.80      0.88      0.84      1000\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.83      0.83      0.83      2000\n",
      "weighted avg       0.83      0.83      0.83      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performNaiveBayes(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X, weights):\n",
    "    \"\"\"\n",
    "    This method calculates the sigmoid function value for the given inputs\n",
    "    INPUT:\n",
    "        X: input data vector (1,X1,X2,..)\n",
    "        weights: weight vector (w0,w1,w2,...)\n",
    "    OUTPUT:\n",
    "        returns the result of sigmoid function\n",
    "    \"\"\"\n",
    "    weightXproduct = np.dot(X, weights) #calculating w(Transpose).X\n",
    "    return 1/(1 + np.exp(-weightXproduct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(X, y, sigmoid_output):\n",
    "    \"\"\"\n",
    "    This method calculates the gradient of the log-likelihood function (∂L(w)/∂w = (y-z)x where z is the sigmoid function)\n",
    "    INPUT:\n",
    "        X: input data vector (1,X1,X2,..)\n",
    "        y: class label\n",
    "        sigmoid_output: result of the sigmoid function calculated for X\n",
    "    OUTPUT:\n",
    "        returns the gradient of the l\n",
    "    \"\"\"\n",
    "    return np.dot(y - sigmoid_output, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(weights, learning_rate, gradient):\n",
    "    \"\"\"\n",
    "    This method updates the weights for the next increment of gradient ascent\n",
    "    INPUT:\n",
    "        weights: weight vector (w0,w1,w2,...)\n",
    "        learning_rate: rate of learning for the gradient ascent method\n",
    "        gradient: gradient of log-likelihood function with respect to the current weight vector\n",
    "    OUTPUT:\n",
    "        returns the updated weight\n",
    "    \"\"\"\n",
    "    return weights + (learning_rate * gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLogisticRegressionModel(X_train, y_train, learning_rate = 0.1, iterations = 10000):\n",
    "    \"\"\"\n",
    "    This method trains the Logistic Regression model using the given training set\n",
    "    INPUT:\n",
    "        X_train: training data with features\n",
    "        y_train: class lables of the training data\n",
    "        learning_rate: rate of learning for the gradient ascent method (default value is 0.1)\n",
    "        iterations: number of iterations to be performed for the gradient ascent method (default value is 10000)\n",
    "    OUTPUT:\n",
    "        returns the weights calculated\n",
    "    \"\"\"\n",
    "    \n",
    "    #Preparing X and y \n",
    "    intercept = np.ones((X_train.shape[0], 1)) # adding 1's to X to correspond to Wo weight\n",
    "    X = np.concatenate((intercept, X_train), axis = 1) #concatenating 1's and training data to form [1, X1, X2, ..]\n",
    "    y = y_train\n",
    "    \n",
    "    #Initializing weights to 0\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    \n",
    "    #Iterations\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        #Preparing inputs for updating weights\n",
    "        sigmoid_output = sigmoid(X, weights)\n",
    "        gradient = calculate_gradient(X, y, sigmoid_output)\n",
    "        \n",
    "        #Gradient Ascent - Updating weights\n",
    "        weights = update_weights(weights, learning_rate, gradient)\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testLogisticRegressionModel(X_test, weights):\n",
    "    \"\"\"\n",
    "    This method performs the testing of the trained Logistic Regression model\n",
    "    INPUT:\n",
    "        X_test: testing dataset with all the features\n",
    "        weights: weight vector (w0,w1,w2,...) of the trained model\n",
    "    OUTPUT:\n",
    "        returns the predictions of all the test data samples\n",
    "    \"\"\"\n",
    "    intercept = np.ones((X_test.shape[0], 1)) # adding 1's to X to correspond to Wo weight\n",
    "    testData = np.concatenate((intercept, X_test), axis = 1) #concatenating 1's and training data to form [1, X1, X2, ..]\n",
    "    \n",
    "    sigmoid_output = sigmoid(testData, weights)\n",
    "\n",
    "    predictions = []\n",
    "    for res in sigmoid_output:\n",
    "        if res > 0.5:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performLogisticRegression(X_train, y_train, X_test, y_test, learning_rate = 0.1, iterations = 10000):\n",
    "    \"\"\"\n",
    "    This method performs the training and testing of Logistic Regression model\n",
    "    INPUT:\n",
    "        X_train: training dataset with features\n",
    "        y_train: class lables of the training dataset\n",
    "        X_test: testing dataset with all the features\n",
    "        y_test: class labels of the testing dataset\n",
    "        learning_rate: rate of learning for the gradient ascent method (default value is 0.1)\n",
    "        iterations: number of iterations to be performed for the gradient ascent method (default value is 10000)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print('\\n--- LOGISTIC REGRESSION ---')\n",
    "    \n",
    "    #Training the model by updating weights using Gradient Ascent technique\n",
    "    weights = trainLogisticRegressionModel(X_train, y_train, learning_rate, iterations)\n",
    "    \n",
    "    #Testing the model using the updated weights\n",
    "    predictions = testLogisticRegressionModel(X_test, weights)\n",
    "    \n",
    "    #Getting the model performance\n",
    "    printModelPerformance(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LOGISTIC REGRESSION ---\n",
      "\n",
      "The overall accuracy of the trained model is 92.2%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[928  72]\n",
      " [ 84 916]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.93      0.92      1000\n",
      "         1.0       0.93      0.92      0.92      1000\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.92      0.92      0.92      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performLogisticRegression(X_train, y_train, X_test, y_test, learning_rate = 0.1, iterations = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
